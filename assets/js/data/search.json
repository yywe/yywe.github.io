[ { "title": "How to Persist Your SSH Session in Remote Server", "url": "/posts/how-to-persist-your-ssh-session-in-remote-server/", "categories": "Linux", "tags": "linux, development environment", "date": "2023-07-29 08:55:00 -0400", "snippet": "One of the major advatanage of developing server side applications in your local enviornment is that the terminal session never gets disconnected. The session is always available.➜ yywe.github.io git:(master) ✗ lsGemfile _javascript assetsGemfile.lock _layouts gulpfile.jsLICENSE _plugins index.htmlREADME.md _posts jekyll-theme-chirpy.gemspec_config.yml _sass note.txt_data _site package.json_includes _tabs toolsHowever, in many companies, it is daily work to SSH into a server and do the development work, and it is annoying that you often see that your session session get disconnected.client_loop: send disconnect: Broken pipeThis may happen while you are having launch or due to network issue or other whatever reason. The consequence is that your onging work/process will be terminated due to this closed session.To solve the problem, I have known the “screen” command for a long time, which can decouple the server side processes with the console window. However, it is still normal that you SSH session may be disconnected and you need to re-connect and attach the your screen session.Is there any way to make your SSH session in a remote server never gets lost and work like your local terminal session. Probably yes! In this post, I will introduce how to achieve this goal. The basic idea is to combine “autossh” and “screen”.If you are not familar with autossh and screen, you might want to learn some basics before continue reading. Usually screen is already installed in the remote server, and you need to install autossh in your local machine.Let’s make your remote SSH session like your local terminal session and never gets lost!1. Prepare your screen config.This is simple, vim ~/.screenrc and put below line:termcapinfo xterm* ti@:te@in the file. If you do not do this, when you scroll your mouse wheel you cannot view your historical commands and output.2. Write the below shell script to a file like myssh.sh# get your local terminal session id (macos, other platform not verified)session_id=$(echo $TERM_SESSION_ID | cut -d':' -f2)# set your usernameuser_id=&lt;username&gt;# the host you will be connecting, will be passed as a parameterhost=$1if [ -z $host ]; then    echo \"please specify the host\"    exitfiecho \"autossh connect to $host using sessionid=$session_id\"# key command, explanation will be followed. autossh -M 0  -o TCPKeepAlive=yes -o ServerAliveCountMax=20 -o ServerAliveInterval=30 $user_id@$host -t screen -d -R $session_idExplanation -M 0: you can specify autossh to open extra TCP port to monitor your ssh session, but in practice I find that is not a must, can use -M 0 so we do not open extra ports in your server. -o TCPKeepAlive=yes -o ServerAliveCountMax=20 -o ServerAliveInterval=30 : This is the first layer of protection, the options specified here will try to keep your SSH session alive by sending heartbeat information at given interval. This is optional as we will have another two mechanism to make your session persistant. screen -d -R $session_id: -R $session_id will try to attach to a screen session named $session_id if exists, if not exists, it will create a session named $session_id. However, it may happen that one ssh connection is disconnected but the server is not refreshed (detached), when autossh reconnect, since it is still attached, it will start a new screen session with different process id and you will end up like below: 4028118.4A435FE8-E9D4-42E8-A40F-FCCFF5C198C2    (07/29/2023 01:13:37 AM)        (Attached)3802470.4A435FE8-E9D4-42E8-A40F-FCCFF5C198C2    (07/28/2023 05:43:22 PM)        (Detached) you will have 2 processes using the same session id! That is why -d comes into the picture. with -d, it will first detach any existing one first. So you will eventually have 1 session binded to your local terminal window. now your session will be under the protection of screen, so you will never lose your session context. Lastly, if for whatever reason like network issue, your connection get lost, autossh will reconnect and reattach to the dedicated screen session.3. Usagewith the above script, if you want to ssh into your remote server, do:~/myssh.sh myserver.mydomain.com4. TipsNow you will have your SSH session persisted, and you will never (hopefully) have your SSH session disconnected unexpectedly. However, if you closed your terminal window, your screen session may still be there. So you may need to constantly monior your screen sessions. Put below lines in your server’s ~/.bashrcalias sls='screen -ls'function sk(){    sname=$1    if [ -z $sname ]; then        echo \"require session name\"        return    fi    screen -S $sname -X quit}With the setting, you can use sls to list existing screen sessions. If you do not want any of them, do sk session_name to quit the session.Now you are enpowered to use your remote server like they are your local machine without need to worry about the annoying disconnection issue." }, { "title": "How to Traverse Trees in Rust", "url": "/posts/how-to-traverse-trees-in-rust/", "categories": "Rust", "tags": "rust", "date": "2023-07-16 08:55:00 -0400", "snippet": "Today learned some something new about Rust when trying to use Rust to traverse binary trees. There are some pattern differences while using Rust to traverse binary trees compared with other languages. One main difference is that Rust does not provide NULL (or Nil) pointer. The alternative to refer to a case without children is None of Option. Also the definition of TreeNode is somewhat different.Let’s first look at the tree node definition:pub struct TreeNode&lt;T&gt; { value: T, left: Option&lt;Box&lt;TreeNode&lt;T&gt;&gt;&gt;, right: Option&lt;Box&lt;TreeNode&lt;T&gt;&gt;&gt;,}Here we can see we wrap the TreeNode inside a Box pointer. and outside the pointer we wrapped it in an Option, and we have a generic parameter T. Next, we can implment a utility function to create a leaf node as:impl&lt;T&gt; TreeNode&lt;T&gt; { fn leaf(value: T) -&gt; Option&lt;Box&lt;TreeNode&lt;T&gt;&gt;&gt; { Some(Box::new(Self { value, left: None, right: None, })) }}With that, we can implement a basic inorder traverse function:impl&lt;T&gt; TreeNode&lt;T&gt; { pub fn traverse_inorder1(&amp;self, f: &amp;mut impl FnMut(&amp;T)) { if let Some(left) = &amp;self.left { left.traverse_inorder1(f); } f(&amp;self.value); if let Some(right) = &amp;self.right { right.traverse_inorder1(f); } }}In this version, we implement the traverse_inorder1 function for the TreeNode, note the 2nd parameter is a closure with the form of ` f: &amp;mut impl FnMut(&amp;T)`, it means the function that will visit the value of the node. The function body is like other languages where we use recursion to traverse the tree.So far, although there are some new things using Rust to traverse binary tree, what really makes things intersting is the usage of ControlFlow. In rust core, it is defined as:#[stable(feature = \"control_flow_enum_type\", since = \"1.55.0\")]// ControlFlow should not implement PartialOrd or Ord, per RFC 3058:// https://rust-lang.github.io/rfcs/3058-try-trait-v2.html#traits-for-controlflow#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]pub enum ControlFlow&lt;B, C = ()&gt; { /// Move on to the next phase of the operation as normal. #[stable(feature = \"control_flow_enum_type\", since = \"1.55.0\")] #[lang = \"Continue\"] Continue(C), /// Exit the operation without running subsequent phases. #[stable(feature = \"control_flow_enum_type\", since = \"1.55.0\")] #[lang = \"Break\"] Break(B), // Yes, the order of the variants doesn't match the type parameters. // They're in this order so that `ControlFlow&lt;A, B&gt;` &lt;-&gt; `Result&lt;B, A&gt;` // is a no-op conversion in the `Try` implementation.}In short, it provide one way to either continue something or break something. This enum has 2 generic parameters. B is for the case when we want to break the execution and return its value (type B), while for the case of continue execution, since usually no return value, so usually C=().Now given one requirement, inorder traverse the binary tree and stop whenever hit a negative node. Then what should we do?. Taking advantage of ControlFlow, we can change the closure like below: pub fn traverse_inorder2&lt;B&gt;(&amp;self, f: &amp;mut impl FnMut(&amp;T) -&gt; ControlFlow&lt;B&gt;) -&gt; ControlFlow&lt;B&gt; { if let Some(left) = &amp;self.left { left.traverse_inorder2(f)?; } f(&amp;self.value)?; if let Some(right) = &amp;self.right { right.traverse_inorder2(f)?; } ControlFlow::Continue(()) }Here the function signature changed to pub fn traverse_inorder2&lt;B&gt;(&amp;self, f: &amp;mut impl FnMut(&amp;T) -&gt; ControlFlow&lt;B&gt;) -&gt; ControlFlow&lt;B&gt; , here we can the generic parameter B, which indicates the type to return when break (assume hit negative node), and the closure also return the ControlFlow, so as the whole function.The whole test code is below:use std::ops::ControlFlow;pub struct TreeNode&lt;T&gt; { value: T, left: Option&lt;Box&lt;TreeNode&lt;T&gt;&gt;&gt;, right: Option&lt;Box&lt;TreeNode&lt;T&gt;&gt;&gt;,}impl&lt;T&gt; TreeNode&lt;T&gt; { pub fn traverse_inorder1(&amp;self, f: &amp;mut impl FnMut(&amp;T)) { if let Some(left) = &amp;self.left { left.traverse_inorder1(f); } f(&amp;self.value); if let Some(right) = &amp;self.right { right.traverse_inorder1(f); } } pub fn traverse_inorder2&lt;B&gt;(&amp;self, f: &amp;mut impl FnMut(&amp;T) -&gt; ControlFlow&lt;B&gt;) -&gt; ControlFlow&lt;B&gt; { if let Some(left) = &amp;self.left { left.traverse_inorder2(f)?; } f(&amp;self.value)?; if let Some(right) = &amp;self.right { right.traverse_inorder2(f)?; } ControlFlow::Continue(()) } fn leaf(value: T) -&gt; Option&lt;Box&lt;TreeNode&lt;T&gt;&gt;&gt; { Some(Box::new(Self { value, left: None, right: None, })) }}fn main() { let node = TreeNode { value: 0, left: TreeNode::leaf(1), right: Some(Box::new(TreeNode { value: -1, left: TreeNode::leaf(5), right: TreeNode::leaf(2), })), }; let mut sum1 = 0; node.traverse_inorder1(&amp;mut |val| { sum1 += val; }); println!(\"The sum1 is {}\", sum1); let mut sum2 = 0; let res = node.traverse_inorder2(&amp;mut |val| { if *val &lt; 0 { ControlFlow::Break(*val) } else { sum2 += *val; ControlFlow::Continue(()) } }); println!(\"The sum2 is {}\", sum2); println!(\"The res is {:?}\", res);}So, to break the execution when hit a negative node, we pass&amp;mut |val| { if *val &lt; 0 { ControlFlow::Break(*val) } else { sum2 += *val; ControlFlow::Continue(()) } }this closure to the function. This is an interesting way to control execution flow in Rust." }, { "title": "Kubernetes ReplicaSet Controller Design Principle", "url": "/posts/kubernetes-replicatset-controller/", "categories": "Kubernetes, Source Code", "tags": "kubernetes, debug, source code", "date": "2022-04-27 08:55:00 -0400", "snippet": "In kubernetes, controllers play a vital role to orchestrate resources. For beginners learning kubernetes, we need to understand what are controllers. In short, controllers manipulate resources. In particular, resources indicate a collection of static object definition, and they exist in the ETCD database (managed by ApiServer). For a simple example, you can throw the below pod yaml fileapiVersion: v1kind: Podmetadata: name: nginxspec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80into ETCD using kubectl, then you have a Pod resource. However, before scheduler arranges it to run in some node and kubelet starts the runtime container, the resource is just a static definition. This sounds like an order for a meal, it is just there and may not be cooked/served yet. The role of controllers is to orchestrate a lot of orders (but controllers do not cook or serve dishes). As another example, say if you throw a ReplicaSet with 3 replicas into ETCD, replicatset controller will then create 3 corresponding Pod resources in ETCD. Again they just make the order, the actual work to launch containers is the responsibility of kubelet. As such, you can immagine that controller development/debug does not require container runtime at all.ReplicaSet controller in k8s is a very basic controller. In this post, we will have a look at the implementation details for ReplicatSet controller.Run ReplicatSet ControllerEarlier I shared a post on how to debug k8s source code. However, that solution is too cubersome. In fact, we have a lighter way if we just want to run/debug simple function unit. Let’s go the the k8s source code folder and run:kubernetes git:(master) go test -v ./pkg/controller/replicaset -run TestWatchPods=== RUN TestWatchPodsI0427 10:34:13.498307 65353 replica_set.go:205] Starting replicaset controllerI0427 10:34:13.498605 65353 shared_informer.go:255] Waiting for caches to sync for ReplicaSetI0427 10:34:13.498619 65353 shared_informer.go:262] Caches are synced for ReplicaSet--- PASS: TestWatchPods (0.00s)PASSok \tk8s.io/kubernetes/pkg/controller/replicaset\t0.789sNow, you see we started a replicaset controller and did a test. The details of this test can be found ./pkg/controller/replicaset/replicat_set_test.go. You can also do a step-by-step debug using:➜ kubernetes git:(master) dlv test ./pkg/controller/replicaset/Type 'help' for list of commands.(dlv) b TestWatchPodsBreakpoint 1 set at 0x2afd2d2 for k8s.io/kubernetes/pkg/controller/replicaset.TestWatchPods() ./pkg/controller/replicaset/replica_set_test.go:708(dlv) c=&gt; 708:\tfunc TestWatchPods(t *testing.T) { 709:\t\tclient := fake.NewSimpleClientset() 710: 711:\t\tfakeWatch := watch.NewFake() 712:\t\tclient.PrependWatchReactor(\"pods\", core.DefaultWatchReactor(fakeWatch, nil))Then you will be able to go through the steps of creating the controller and see how it works.ReplicaSet Controller Design PrincipleThe ultimate goal of replicat set controller is to ensure there are always a dedicated number of pods (here we can only ensure from the resource manifest level, but we cannot guarantee cooresponding containers can be created). Before we have look at the implementation, it is helpful if we ask the following questions:How to deal with the case when controllers get restarted?Cause the controllers may be interrupted but the number of pods is not enough. The overview is count the existing number of pods at the begining of every sync loop. Each managed pod is expected to have the same labels and may have a owner reference points to the replicat set. Then depending on the number, we keep creating or deleting pods to statisfy the dedidated number.Implementation DetailsWe will take a look at a few key implementation details. The ReplicatSetController is defined as a struct:type ReplicaSetController struct {\tschema.GroupVersionKind\tkubeClient clientset.Interface\t //This is podController interface, which will be responsible to create or delete pod //resources.\tpodControl controller.PodControlInterface\t\tburstReplicas int\tsyncHandler func(ctx context.Context, rsKey string) error\t// Think about what if we want to create 200 Pods in one sync, but before it finishes\t// a second sync is started? Here in the first sync, expecations will say I expect to \t// create 200 pods. Thus if the expections are not satisfied, second sync will quit\texpectations *controller.UIDTrackingControllerExpectations\trsLister appslisters.ReplicaSetLister\trsListerSynced cache.InformerSynced\t //This is storage for objects\trsIndexer cache.Indexer\tpodLister corelisters.PodLister\t\tpodListerSynced cache.InformerSynced\t// This is work queue. items in this queue will be processed one by one\tqueue workqueue.RateLimitingInterface}When the ReplicatSet controller is created, it will listen to Replicat Set and Pods Add/Update/Delete Events.func NewBaseController(rsInformer appsinformers.ReplicaSetInformer, podInformer coreinformers.PodInformer, kubeClient clientset.Interface, burstReplicas int,\tgvk schema.GroupVersionKind, metricOwnerName, queueName string, podControl controller.PodControlInterface) *ReplicaSetController {\tif kubeClient != nil &amp;&amp; kubeClient.CoreV1().RESTClient().GetRateLimiter() != nil {\t\tratelimiter.RegisterMetricAndTrackRateLimiterUsage(metricOwnerName, kubeClient.CoreV1().RESTClient().GetRateLimiter())\t} ...\trsInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{\t\tAddFunc: rsc.addRS,\t\tUpdateFunc: rsc.updateRS,\t\tDeleteFunc: rsc.deleteRS,\t})\t...\t\tpodInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{\t\tAddFunc: rsc.addPod,\t\t// This invokes the ReplicaSet for every pod change, eg: host assignment. Though this might seem like\t\t// overkill the most frequent pod update is status, and the associated ReplicaSet will only list from\t\t// local storage, so it should be ok.\t\tUpdateFunc: rsc.updatePod,\t\tDeleteFunc: rsc.deletePod,\t})For which ever detected, the corresponding action is to add the respective ReplicatSet resource into the work queue. For instance:func (rsc *ReplicaSetController) addRS(obj interface{}) {\trs := obj.(*apps.ReplicaSet)\tklog.V(4).Infof(\"Adding %s %s/%s\", rsc.Kind, rs.Namespace, rs.Name)\trsc.enqueueRS(rs)}func (rsc *ReplicaSetController) enqueueRS(rs *apps.ReplicaSet) {\tkey, err := controller.KeyFunc(rs)\tif err != nil {\t\tutilruntime.HandleError(fmt.Errorf(\"couldn't get key for object %#v: %v\", rs, err))\t\treturn\t}\trsc.queue.Add(key)}Now we can have a look at the control loop. For each worker process, it will take one item from the work queue and call syncHandler to start a sync loop.func (rsc *ReplicaSetController) processNextWorkItem(ctx context.Context) bool {\tkey, quit := rsc.queue.Get()\tif quit {\t\treturn false\t}\tdefer rsc.queue.Done(key)\terr := rsc.syncHandler(ctx, key.(string))\tif err == nil {\t\trsc.queue.Forget(key)\t\treturn true\t}\tutilruntime.HandleError(fmt.Errorf(\"sync %q failed with %v\", key, err))\trsc.queue.AddRateLimited(key)\treturn true}Here syncHandler points to syncReplicaSet://Sync a specific replicat set, key is the fullname for the replicatsetfunc (rsc *ReplicaSetController) syncReplicaSet(ctx context.Context, key string) error {\t... // Get namespace and anme\tnamespace, name, err := cache.SplitMetaNamespaceKey(key) // Get the replicatset object\trs, err := rsc.rsLister.ReplicaSets(namespace).Get(name) // Check if the last round of sync is satisfied (finished)\trsNeedsSync := rsc.expectations.SatisfiedExpectations(key)\t // First get all the pods in that namespace\tallPods, err := rsc.podLister.Pods(rs.Namespace).List(labels.Everything())\t// Ignore inactive pods.\tfilteredPods := controller.FilterActivePods(allPods) // Now get all existing pods for the replicatset\tfilteredPods, err = rsc.claimPods(ctx, rs, selector, filteredPods)\tvar manageReplicasErr error\tif rsNeedsSync &amp;&amp; rs.DeletionTimestamp == nil {\t // Now we depends on how many existing pods // either create new pods or delete pods\t\tmanageReplicasErr = rsc.manageReplicas(ctx, filteredPods, rs)\t}\t....The actual work to create new pods or delete pods will be done in manageReplicas.func (rsc *ReplicaSetController) manageReplicas(ctx context.Context, filteredPods []*v1.Pod, rs *apps.ReplicaSet) error { //Caclulate the difference, either to create a delete\tdiff := len(filteredPods) - int(*(rs.Spec.Replicas))\trsKey, err := controller.KeyFunc(rs)\t...\tif diff &lt; 0 {\t\tdiff *= -1\t\tif diff &gt; rsc.burstReplicas {\t\t\tdiff = rsc.burstReplicas\t\t}\t\t// Set the number of pods expected to create, for each pod \t\t// that is created succefully, this number will decrease 1.\t\t// When pod add event is detected, there is a call to \t\t// rsc.expectations.CreationObserved(rsKey) to reduce this number\t\t// The next round of sync will quit if this number is not 0.\t\trsc.expectations.ExpectCreations(rsKey, diff)\t\tklog.V(2).InfoS(\"Too few replicas\", \"replicaSet\", klog.KObj(rs), \"need\", *(rs.Spec.Replicas), \"creating\", diff)\t\t\t\tsuccessfulCreations, err := slowStartBatch(diff, controller.SlowStartInitialBatchSize, func() error {\t\t // Actual create pods work is delegated to podControl\t\t\terr := rsc.podControl.CreatePods(ctx, rs.Namespace, &amp;rs.Spec.Template, rs, metav1.NewControllerRef(rs, rsc.GroupVersionKind))\t\t\tif err != nil {\t\t\t\tif apierrors.HasStatusCause(err, v1.NamespaceTerminatingCause) {\t\t\t\t\treturn nil\t\t\t\t}\t\t\t}\t\t\treturn err\t\t}) ....\t}Actual pods creation work is wrapped in slowStartBatch, there pods will be created batch by batch with different batch sizes. errCh := make(chan error, batchSize)\t\tvar wg sync.WaitGroup\t\twg.Add(batchSize)\t\tfor i := 0; i &lt; batchSize; i++ {\t\t\tgo func() {\t\t\t\tdefer wg.Done()\t\t\t\t//Here fn is the function passed from the parameter of slowStartBatch\t\t\t\tif err := fn(); err != nil {\t\t\t\t\terrCh &lt;- err\t\t\t\t}\t\t\t}()\t\t}\t\twg.Wait()SummaryIn this post, we had an overview of the principle of replicat set controller in k8s. At each sync loop, the controller will claim pods (i.e., count how many pods exists under the replicat set), then decide whether to create more pods or delete pods. There are some hightlight implementation tricks to pay attention, such as use expectations to avoid concurrent sync loop, and use slow start batch to make pod creation process smooth." }, { "title": "A Better Way to Debug TiDB Source Code", "url": "/posts/a-better-way-to-debug-tidb-source-code/", "categories": "Database, TiDB", "tags": "TiDB, debug", "date": "2022-04-08 08:55:00 -0400", "snippet": "TiDB is an interesting SQL database written in Golang. One major difference compared with popular exsiting relational databases is that in TiDB storage is separated as an independant runtime. The database (SQL) layer will call the storage layer to access data.For database learners, most of us actually will only be interest in the SQL part, i.e, from SQL statment in string form how we can get the query result. Although other parts like server or authtication are also essential parts for a database product, we do not really care about them if we just need to learn database principles.The official document demenstrated the whole lifecyle of an SQL statement in the production environment, and we can follow this guide to debug TiDB.However, the above debug process is cubersome. You will launch the database server, use MySQL client to send SQL querys. This is not really nessesary if we just want to track the execution of a single SQL statement. Meanwhile, when you set a break point and want to observe your expected SQL statement, the debugger will also break if it encounter some other statements triggered by the server itself or some other components, this is annoying. Is there a better way to debug a single SQL statement?That is the motivation for this post. We will show how we can achieve this using the below code./*A simple Go program aims for better debugging SQL execution process in TiDB.Thus you can focus on Parsing, OPtimization and Execution without distractionfrom the server part and connecton part. In addtion, no MySQL client is required.Usuage:1. Go to the tidb source folder, and create a folder like 'sqlexec', and copy the following code into sqlexec/main.go2. Compile and execute:go build sqlexec/main.go &amp;&amp; ./main3. Debug:dlv debug sqlexec/main.goEnjoy!*/package mainimport (\t\"context\"\t\"fmt\"\t\"github.com/pingcap/tidb/parser/terror\"\t\"github.com/pingcap/tidb/session\"\tkvstore \"github.com/pingcap/tidb/store\"\t\"github.com/pingcap/tidb/store/mockstore\")func main() {\t//1. Prepare storage, use your existing store path, tables are there\tstorePath := \"unistore:///tmp/tidb\"\terr := kvstore.Register(\"unistore\", mockstore.EmbedUnistoreDriver{})\tterror.MustNil(err)\tstorage, err := kvstore.New(storePath)\tterror.MustNil(err)\t//2. Prepare a session and set the working database\tse, err := session.CreateSession4Test(storage)\tterror.MustNil(err)\tse.GetSessionVars().CurrentDB = \"test\"\tterror.MustNil(err)\t//3. The sql statement you want to execute\t//sql := \"insert into t1 values(5,'lucy','newyork','female')\"\tsql := \"select * from t1 where id &lt;=3\"\t//4. Parse and Execute.\tctx := context.Background()\tstmts, err := se.Parse(ctx, sql)\tterror.MustNil(err)\trs, err := se.ExecuteStmt(ctx, stmts[0])\tterror.MustNil(err)\t//5. Get result for select statement\tif rs != nil {\t\tsRows, err := session.ResultSetToStringSlice(ctx, se, rs)\t\tterror.MustNil(err)\t\tfmt.Println(\"Query Result:\", sRows)\t} else {\t\tfmt.Println(\"Execution Succeed\")\t}}go build sqlexec/main.go &amp;&amp; ./main[2022/04/08 08:29:01.448 -04:00] [INFO] [store.go:74] [\"new store\"] [path=unistore:///tmp/tidb][2022/04/08 08:29:01.493 -04:00] [INFO] [db.go:143] [\"replay wal\"] [\"first key\"=7580000000000000255f728000000000000001f9ffe9defe5fffff(432369895244562432)][2022/04/08 08:29:01.494 -04:00] [INFO] [store.go:80] [\"new store with retry success\"][2022/04/08 08:29:01.494 -04:00] [INFO] [tidb.go:72] [\"new domain\"] [store=6ebcd41f-95db-42e6-a785-e40ae59d567b] [\"ddl lease\"=1s] [\"stats lease\"=3s] [\"index usage sync lease\"=0s][2022/04/08 08:29:01.503 -04:00] [INFO] [domain.go:169] [\"full load InfoSchema success\"] [currentSchemaVersion=0] [neededSchemaVersion=37] [\"start time\"=5.741453ms][2022/04/08 08:29:01.503 -04:00] [INFO] [domain.go:432] [\"full load and reset schema validator\"][2022/04/08 08:29:01.503 -04:00] [INFO] [ddl.go:366] [\"[ddl] start DDL\"] [ID=c7216a3b-520a-4471-86fe-080e30c4d09b] [runWorker=true][2022/04/08 08:29:01.504 -04:00] [INFO] [ddl.go:355] [\"[ddl] start delRangeManager OK\"] [\"is a emulator\"=true][2022/04/08 08:29:01.504 -04:00] [WARN] [sysvar_cache.go:54] [\"sysvar cache is empty, triggering rebuild\"][2022/04/08 08:29:01.504 -04:00] [INFO] [delete_range.go:142] [\"[ddl] start delRange emulator\"][2022/04/08 08:29:01.504 -04:00] [INFO] [ddl_worker.go:156] [\"[ddl] start DDL worker\"] [worker=\"worker 1, tp general\"][2022/04/08 08:29:01.504 -04:00] [INFO] [ddl_worker.go:156] [\"[ddl] start DDL worker\"] [worker=\"worker 2, tp add index\"]Query Result: [[1 zhangsan chongqing male] [2 lishi beijing female] [3 wang shandong male]]With the above code, you can perfectly dive into the internals of SQL execution process, no need to worry about server start, connect with MySQL client, etc. After get familar with the the three essential steps (Parser, Planner, Executor, this may take some time), we can then go back to learn other components." }, { "title": "How to Debug Kubernetes Source Code?", "url": "/posts/how-to-debug-k8s-sourceCode/", "categories": "Kubernetes, Source Code", "tags": "kubernetes, debug, source code", "date": "2022-04-01 08:55:00 -0400", "snippet": "Reading the source code of kubernetes is a great way to learn the internals of kubernetes. In this post, we will explore how to use debugger (dlv) to track the source code execution flow.PrerequisiteTo setup a debug environment, there are a few prerequisites. Linux system. A copy of Kubernetes source code (of course). Learn how to use dlv.The very first step is to ensure you are able to run a local single node k8s cluster. With help from the community, this has been simplified. Detailed steps to run a local k8s cluster is documented at https://github.com/kubernetes/community/blob/master/contributors/devel/running-locally.md.Before reading the rest of this post, please go through the steps described in the link and make a running local k8s cluster (even if not for debugging the source, learn this process is helpful to quickly start a k8s cluster for other test purpose. You do not have to install minikube to start a local cluster…).If things work out as expected, try execute:sudo env \"PATH=$PATH\" hack/local-up-cluster.sh -OHere I have env \"PATH=$PATH\" because I installed etcd under my home directory, so I reserved the path environment such that etcd can be found under sudo. This is not nessesary if you have etcd installed under system binary path. Another thing here is -O since I already compiled the source code. This is documented in the link above. You will be able to see something like:To start using your cluster, you can open up another terminal/tab and run: export KUBECONFIG=/var/run/kubernetes/admin.kubeconfig cluster/kubectl.shAlternatively, you can write to the default kubeconfig: export KUBERNETES_PROVIDER=local cluster/kubectl.sh config set-cluster local --server=https://localhost:6443 --certificate-authority=/var/run/kubernetes/server-ca.crt cluster/kubectl.sh config set-credentials myself --client-key=/var/run/kubernetes/client-admin.key --client-certificate=/var/run/kubernetes/client-admin.crt cluster/kubectl.sh config set-context local --cluster=local --user=myself cluster/kubectl.sh config use-context local cluster/kubectl.shGreat! Now you are able to start a local k8s cluster from source code! (please also set the kubeconfig as prompted, export KUBECONFIG=/var/run/kubernetes/admin.kubeconfig). Now let’s first look at what is happenning under the hood.ps -ef|grep kuberoot 3731 3213 16 15:20 pts/6 00:00:21 ~/kubernetes/_output/bin/kube-apiserver --authorization-mode=Node,RBAC --cloud-provider= --cloud-config= --v=3 --vmodule= --audit-policy-file=/tmp/kube-audit-policy-file --audit-log-path=/tmp/kube-apiserver-audit.log --authorization-webhook-config-file= --authentication-token-webhook-config-file= --cert-dir=/var/run/kubernetes --egress-selector-config-file=/tmp/kube_egress_selector_configuration.yaml --client-ca-file=/var/run/kubernetes/client-ca.crt --kubelet-client-certificate=/var/run/kubernetes/client-kube-apiserver.crt --kubelet-client-key=/var/run/kubernetes/client-kube-apiserver.key --service-account-key-file=/tmp/kube-serviceaccount.key --service-account-lookup=true --service-account-issuer=https://kubernetes.default.svc --service-account-jwks-uri=https://kubernetes.default.svc/openid/v1/jwks --service-account-signing-key-file=/tmp/kube-serviceaccount.key --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,Priority,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota --disable-admission-plugins= --admission-control-config-file= --bind-address=0.0.0.0 --secure-port=6443 --tls-cert-file=/var/run/kubernetes/serving-kube-apiserver.crt --tls-private-key-file=/var/run/kubernetes/serving-kube-apiserver.key --storage-backend=etcd3 --storage-media-type=application/vnd.kubernetes.protobuf --etcd-servers=http://127.0.0.1:2379 --service-cluster-ip-range=10.0.0.0/24 --feature-gates=AllAlpha=false --external-hostname=localhost --requestheader-username-headers=X-Remote-User --requestheader-group-headers=X-Remote-Group --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-client-ca-file=/var/run/kubernetes/request-header-ca.crt --requestheader-allowed-names=system:auth-proxy --proxy-client-cert-file=/var/run/kubernetes/client-auth-proxy.crt --proxy-client-key-file=/var/run/kubernetes/client-auth-proxy.key --cors-allowed-origins=/127.0.0.1(:[0-9]+)?$,/localhost(:[0-9]+)?$root 4197 3213 3 15:20 pts/6 00:00:04 ~/kubernetes/_output/bin/kube-controller-manager --v=3 --vmodule= --service-account-private-key-file=/tmp/kube-serviceaccount.key --service-cluster-ip-range=10.0.0.0/24 --root-ca-file=/var/run/kubernetes/server-ca.crt --cluster-signing-cert-file=/var/run/kubernetes/client-ca.crt --cluster-signing-key-file=/var/run/kubernetes/client-ca.key --enable-hostpath-provisioner=false --pvclaimbinder-sync-period=15s --feature-gates=AllAlpha=false --cloud-provider= --cloud-config= --configure-cloud-routes=true --authentication-kubeconfig /var/run/kubernetes/controller.kubeconfig --authorization-kubeconfig /var/run/kubernetes/controller.kubeconfig --kubeconfig /var/run/kubernetes/controller.kubeconfig --use-service-account-credentials --controllers=* --leader-elect=false --cert-dir=/var/run/kubernetes --master=https://localhost:6443root 4200 3213 1 15:20 pts/6 00:00:01 ~/kubernetes/_output/bin/kube-scheduler --v=3 --config=/tmp/kube-scheduler.yaml --feature-gates=AllAlpha=false --authentication-kubeconfig /var/run/kubernetes/scheduler.kubeconfig --authorization-kubeconfig /var/run/kubernetes/scheduler.kubeconfig --master=https://localhost:6443root 4487 3213 0 15:20 pts/6 00:00:00 sudo -E ~/kubernetes/_output/bin/kubelet --v=3 --vmodule= --container-runtime=docker --hostname-override=127.0.0.1 --cloud-provider= --cloud-config= --bootstrap-kubeconfig=/var/run/kubernetes/kubelet.kubeconfig --kubeconfig=/var/run/kubernetes/kubelet-rotated.kubeconfig --config=/tmp/kubelet.yamlroot 4492 4487 7 15:20 pts/6 00:00:09 ~/kubernetes/_output/bin/kubelet --v=3 --vmodule= --container-runtime=docker --hostname-override=127.0.0.1 --cloud-provider= --cloud-config= --bootstrap-kubeconfig=/var/run/kubernetes/kubelet.kubeconfig --kubeconfig=/var/run/kubernetes/kubelet-rotated.kubeconfig --config=/tmp/kubelet.yamlroot 6203 3213 0 15:20 pts/6 00:00:00 sudo ~/kubernetes/_output/bin/kube-proxy --v=3 --config=/tmp/kube-proxy.yaml --master=https://localhost:6443root 6550 6203 0 15:20 pts/6 00:00:00 ~/kubernetes/_output/bin/kube-proxy --v=3 --config=/tmp/kube-proxy.yaml --master=https://localhost:6443These processess are the components of a k8s cluster. Namely, we have the apiserver, controller manager, scheduler, kubelet and proxy. Note that here for kubelet and proxy, we have two processes for each. I guess this is to simulate the case in worker node, we only run kubelet and proxy, and the other pair is assumed to be in master node.Debug kubelet1. Launch ProcessTo debug kubelet, we will first compile a copy of the kubelet with debug infomation. This can be done by make GOLDFLAGS=\"\" WHAT=\"cmd/kubelet\".Now we kill the process “sudo -E ~/kubernetes/_output/bin/kubelet….”. For the above example, we sudo kill 4487. After that, we restart the process using dlv:sudo env \"PATH=$PATH\" dlv exec ~/kubernetes/_output/bin/kubelet -- --v=3 --vmodule= --container-runtime=docker --hostname-override=127.0.0.1 --cloud-provider= --cloud-config= --bootstrap-kubeconfig=/var/run/kubernetes/kubelet.kubeconfig --kubeconfig=/var/run/kubernetes/kubelet-rotated.kubeconfig --config=/tmp/kubelet.yamlType 'help' for list of commands.(dlv)We simply copy the original starting parameters and add prefix sudo env \"PATH=$PATH\" dlv exec, as such, it will use dlv to start the process. Be very careful with the format when using dlv to pass parameters with --.Now we can set a break point at main.main, and debug like any other programs using dlv:(dlv) b main.mainBreakpoint 1 set at 0x3766166 for main.main() _output/local/go/src/k8s.io/kubernetes/cmd/kubelet/kubelet.go:39(dlv) c2022-03-31T22:47:20-05:00 error layer=debugger error loading binary \"/lib/x86_64-linux-gnu/libpthread.so.0\": could not parse .eh_frame section: unknown CIE_id 0x9daad7e4 at 0x0&gt; main.main() _output/local/go/src/k8s.io/kubernetes/cmd/kubelet/kubelet.go:39 (hits goroutine(1):1 total:1) (PC: 0x3766166)Warning: debugging optimized function 34: _ \"k8s.io/component-base/metrics/prometheus/restclient\" 35: _ \"k8s.io/component-base/metrics/prometheus/version\" // for version metric registration 36: \"k8s.io/kubernetes/cmd/kubelet/app\" 37: ) 38:=&gt; 39: func main() { 40: command := app.NewKubeletCommand() 41: 42: // kubelet uses a config file and does its own special 43: // parsing of flags and that config file. It initializes 44: // logging after it is done with that. Therefore it does(dlv)One thing to note, please do every command at the source code direcotry to avoid any source code loading issues.Bascially, all the commands for k8s using Cobra command. For kubelet, the eventual entry point is located at app.Run, we can set another break point and go there:(dlv) b app.RunBreakpoint 2 set at 0x375f292 for k8s.io/kubernetes/cmd/kubelet/app.Run() ./_output/local/go/src/k8s.io/kubernetes/cmd/kubelet/app/server.go:444(dlv) cFlag --cloud-provider has been deprecated, will be removed in 1.23, in favor of removing cloud provider code from Kubelet.Flag --cloud-config has been deprecated, will be removed in 1.23, in favor of removing cloud provider code from Kubelet.Flag --cloud-provider has been deprecated, will be removed in 1.23, in favor of removing cloud provider code from Kubelet.Flag --cloud-config has been deprecated, will be removed in 1.23, in favor of removing cloud provider code from Kubelet.I0331 23:08:16.519486 26991 mount_linux.go:222] Detected OS with systemd&gt; k8s.io/kubernetes/cmd/kubelet/app.Run() ./_output/local/go/src/k8s.io/kubernetes/cmd/kubelet/app/server.go:444 (hits goroutine(1):1 total:1) (PC: 0x375f292)Warning: debugging optimized function 439: 440: // Run runs the specified KubeletServer with the given Dependencies. This should never exit. 441: // The kubeDeps argument may be nil - if so, it is initialized from the settings on KubeletServer. 442: // Otherwise, the caller is assumed to have set up the Dependencies object and a default one will 443: // not be generated.=&gt; 444: func Run(ctx context.Context, s *options.KubeletServer, kubeDeps *kubelet.Dependencies, featureGate featuregate.FeatureGate) error { 445: // To help debugging, immediately log version 446: klog.InfoS(\"Kubelet version\", \"kubeletVersion\", version.Get()) 447: if err := initForOS(s.KubeletFlags.WindowsService, s.KubeletFlags.WindowsPriorityClass); err != nil { 448: return fmt.Errorf(\"failed OS init: %w\", err) 449: }Up to this point, you can go ahead step by step and see how kubelet is started. If you track all the way down, you will reach startKubelet:(dlv) list&gt; k8s.io/kubernetes/cmd/kubelet/app.RunKubelet() ./_output/local/go/src/k8s.io/kubernetes/cmd/kubelet/app/server.go:1230 (PC: 0x3764ac2)Warning: debugging optimized function 1225: if _, err := k.RunOnce(podCfg.Updates()); err != nil { 1226: return fmt.Errorf(\"runonce failed: %w\", err) 1227: } 1228: klog.InfoS(\"Started kubelet as runonce\") 1229: } else {=&gt;1230: startKubelet(k, podCfg, &amp;kubeServer.KubeletConfiguration, kubeDeps, kubeServer.EnableServer) 1231: klog.InfoS(\"Started kubelet\") 1232: } 1233: return nilThe function startKubelet is the final starting point. After line 1230, kubelet will be started and working. We can take a look at this entry function:func startKubelet(k kubelet.Bootstrap, podCfg *config.PodConfig, kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *kubelet.Dependencies, enableServer bool) {\t// start the kubelet, actual work is done here\tgo k.Run(podCfg.Updates())\t// start the kubelet server, for HTTP purpose\tif enableServer {\t\tgo k.ListenAndServe(kubeCfg, kubeDeps.TLSOptions, kubeDeps.Auth)\t}\tif kubeCfg.ReadOnlyPort &gt; 0 {\t\tgo k.ListenAndServeReadOnly(netutils.ParseIPSloppy(kubeCfg.Address), uint(kubeCfg.ReadOnlyPort))\t}\tif utilfeature.DefaultFeatureGate.Enabled(features.KubeletPodResources) {\t\tgo k.ListenAndServePodResources()\t}}Here k is actually the Kubelet object returned by createAndInitKubelet, which is a struct contains many components, and kubeCfg is the configuration, and kubeDeps is the dependencies (sounds like a runtime context for kubelet). All the work until this point is just to prepare the runtime context.In startKubelet, the runtime context is ready, so we can eventually make kubelet running. The main function called is k.Run, here it will start each individual compoent such as volume manager, pod lifecycle event generator, etc. Eventually, it goes to a syncLoop, which will start to watch the API server and process pod life cycle events.// Run starts the kubelet reacting to config updatesfunc (kl *Kubelet) Run(updates &lt;-chan kubetypes.PodUpdate) {\tif kl.logServer == nil {\t\tkl.logServer = http.StripPrefix(\"/logs/\", http.FileServer(http.Dir(\"/var/log/\")))\t}\tif kl.kubeClient == nil {\t\tklog.InfoS(\"No API server defined - no node status update will be sent\")\t}\t// Start the cloud provider sync manager\tif kl.cloudResourceSyncManager != nil {\t\tgo kl.cloudResourceSyncManager.Run(wait.NeverStop)\t}\tif err := kl.initializeModules(); err != nil {\t\tkl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.KubeletSetupFailed, err.Error())\t\tklog.ErrorS(err, \"Failed to initialize internal modules\")\t\tos.Exit(1)\t}\t// Start volume manager\tgo kl.volumeManager.Run(kl.sourcesReady, wait.NeverStop)\tif kl.kubeClient != nil {\t\t// Introduce some small jittering to ensure that over time the requests won't start\t\t// accumulating at approximately the same time from the set of nodes due to priority and\t\t// fairness effect.\t\tgo wait.JitterUntil(kl.syncNodeStatus, kl.nodeStatusUpdateFrequency, 0.04, true, wait.NeverStop)\t\tgo kl.fastStatusUpdateOnce()\t\t// start syncing lease\t\tgo kl.nodeLeaseController.Run(wait.NeverStop)\t}\tgo wait.Until(kl.updateRuntimeUp, 5*time.Second, wait.NeverStop)\t// Set up iptables util rules\tif kl.makeIPTablesUtilChains {\t\tkl.initNetworkUtil()\t}\t// Start component sync loops.\tkl.statusManager.Start()\t// Start syncing RuntimeClasses if enabled.\tif kl.runtimeClassManager != nil {\t\tkl.runtimeClassManager.Start(wait.NeverStop)\t}\t// Start the pod lifecycle event generator.\tkl.pleg.Start()\tkl.syncLoop(updates, kl)}Now let’s take a look at syncLoop.// syncLoop is the main loop for processing changes. It watches for changes from// three channels (file, apiserver, and http) and creates a union of them. For// any new change seen, will run a sync against desired state and running state. If// no changes are seen to the configuration, will synchronize the last known desired// state every sync-frequency seconds. Never returns.func (kl *Kubelet) syncLoop(updates &lt;-chan kubetypes.PodUpdate, handler SyncHandler) {\tklog.InfoS(\"Starting kubelet main sync loop\")\t// The syncTicker wakes up kubelet to checks if there are any pod workers\t// that need to be sync'd. A one-second period is sufficient because the\t// sync interval is defaulted to 10s.\tsyncTicker := time.NewTicker(time.Second)\tdefer syncTicker.Stop()\thousekeepingTicker := time.NewTicker(housekeepingPeriod)\tdefer housekeepingTicker.Stop()\tplegCh := kl.pleg.Watch() //The channel to watch POD life cycle events\tconst (\t\tbase = 100 * time.Millisecond\t\tmax = 5 * time.Second\t\tfactor = 2\t)\tduration := base\t// Responsible for checking limits in resolv.conf\t// The limits do not have anything to do with individual pods\t// Since this is called in syncLoop, we don't need to call it anywhere else\tif kl.dnsConfigurer != nil &amp;&amp; kl.dnsConfigurer.ResolverConfig != \"\" {\t\tkl.dnsConfigurer.CheckLimitsForResolvConf()\t}\tfor {\t\tif err := kl.runtimeState.runtimeErrors(); err != nil {\t\t\tklog.ErrorS(err, \"Skipping pod synchronization\")\t\t\t// exponential backoff\t\t\ttime.Sleep(duration)\t\t\tduration = time.Duration(math.Min(float64(max), factor*float64(duration)))\t\t\tcontinue\t\t}\t\t// reset backoff if we have a success\t\tduration = base\t\tkl.syncLoopMonitor.Store(kl.clock.Now())\t\tif !kl.syncLoopIteration(updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) {\t\t\tbreak\t\t}\t\tkl.syncLoopMonitor.Store(kl.clock.Now())\t}}Here, updates is the channel to receive pod update, it will be passed to syncLoopIteration, and there all the pod related events are processed.2. Observe how a pod is createdNext, we first set a break point at syncLoopIteration:(dlv) b b HandlePodAdditionsBreakpoint 7 (enabled) at 0x3725cea for k8s.io/kubernetes/pkg/kubelet.(*Kubelet).HandlePodAdditions() ./_output/local/go/src/k8s.io/kubernetes/pkg/kubelet/kubelet.go:2204 (0)Then we are ready to go. After c command, you will see lots of logs.(dlv) c...d-bpjkz\" containerName=\"coredns\"I0401 12:21:21.597873 22459 kubelet_pods.go:1076] \"Clean up pod workers for terminated pods\"I0401 12:21:21.597923 22459 kubelet_pods.go:1105] \"Clean up probes for terminating and terminated pods\"I0401 12:21:21.604405 22459 kubelet_pods.go:1142] \"Clean up orphaned pod statuses\"I0401 12:21:21.610547 22459 kubelet_pods.go:1161] \"Clean up orphaned pod directories\"I0401 12:21:21.610831 22459 kubelet_pods.go:1172] \"Clean up orphaned mirror pods\"I0401 12:21:21.610858 22459 kubelet_pods.go:1179] \"Clean up orphaned pod cgroups\"...Now open another terminal, and set the KUBECONFIG. after that, we create a deployment: kubectl apply -f https://k8s.io/examples/application/simple_deployment.yaml deployment.apps/nginx-deployment createdThen go back to the debug console, we can see: 2202: // HandlePodAdditions is the callback in SyncHandler for pods being added from 2203: // a config source.=&gt;2204: func (kl *Kubelet) HandlePodAdditions(pods []*v1.Pod) { 2205: start := kl.clock.Now() 2206: sort.Sort(sliceutils.PodsByCreationTime(pods)) 2207: for _, pod := range pods { 2208: existingPods := kl.podManager.GetPods() 2209: // Always add the pod to the pod manager. Kubelet relies on the podHowever, to create a pod, it is not as easy as just call some function and a pod is created there. The process is really really complex. Below is just a summary of the processes. We will stop here for this post. Interested readers can follow the source code and dig more details." }, { "title": "Go Concurrency - Stop Signal and Timeout", "url": "/posts/go-concurrency-stop-signal-and-timeout/", "categories": "Go, Concurrency", "tags": "go, concurrency, goroutine", "date": "2022-03-24 08:55:00 -0400", "snippet": "In this post, we will see some other design patterns with go’s channel. Specifically, we can use a channel to deliever a stop signal to goroutines. Meanwhile, when we are waiting on a channel, we can set a time limit, if after the dedicated time we still cannot get something we want, we can take actions.Stop Goroutines with Stop SignalOne commonly used pattern in Go programming is to use a channel to deliever a stop signal to goroutines, such that the goroutines can be stopped correctly. This can be done simply by closing the channel. Let’s say we have the below Go program called close.go:package mainimport \"fmt\"import \"sync\"import \"time\"func main(){ var wg sync.WaitGroup quit := make(chan bool) for i:=0; i&lt;3; i++ { wg.Add(1) go func(i int){ defer wg.Done() doTask := func(){ fmt.Printf(\"In routine %d, doing the task\\n\",i) time.Sleep(time.Second * 1) } for { select { case _, ok := &lt;-quit: if !ok { fmt.Printf(\"routine %d received quit signal\\n\", i) return } default: doTask() } } }(i) } time.Sleep(time.Second * 5) //Let routines run 5 se close(quit) //Signal quit thus routines will stop wg.Wait() //Wait routines to stop}In this case, we created a channel called quit, to send stop signal we can simply close the channel. Then we started three goroutines, and each routine will monitor if the channel is closed using a select mechanism. If the channel is closed, we return from the goroutine. If the channel if not closed, it will do the task. We also utilized a wait group so the main routine will wait until all the serving goroutines are stopped. Let’s run the program:go run close.goIn routine 2, doing the taskIn routine 0, doing the taskIn routine 1, doing the taskIn routine 1, doing the taskIn routine 2, doing the taskIn routine 0, doing the taskIn routine 2, doing the taskIn routine 0, doing the taskIn routine 1, doing the taskIn routine 2, doing the taskIn routine 0, doing the taskIn routine 1, doing the taskIn routine 1, doing the taskIn routine 2, doing the taskIn routine 0, doing the taskroutine 0 received quit signalroutine 2 received quit signalroutine 1 received quit signalWait on Channel until TimeoutAnother commonly used pattern in Go channel is to wait on the channel for some dedicated time. If after the dedicated time we still cannot receive data from the channel, we can take some actions. For example with timeout.go:package mainimport \"fmt\"import \"sync\"import \"time\"func main(){ var wg sync.WaitGroup waitCh := make(chan int) wg.Add(1) go func(){ fmt.Println(\"I am the go routine waiting on the channel...\") select { case _ = &lt;- waitCh: fmt.Println(\"I received data from the channel.\") //Not reachable case &lt;- time.After(time.Second * 4): fmt.Println(\"Time out, I have waited enough time\") wg.Done() } }() wg.Wait() //Wait routines to stop}Run the above program,we can see the below output.go run timeout.goI am the go routine waiting on the channel...Time out, I have waited enough time" }, { "title": "Go Concurrency - Use Channel to Simulate Mutex", "url": "/posts/go-concurrency-use-channel-to-simulateMutex/", "categories": "Go, Concurrency", "tags": "go, concurrency, goroutine", "date": "2022-03-22 08:55:00 -0400", "snippet": "Go’s has two types of channels, namely unbuffered channel and buffered channel. With unbuffered channel, the sending or receiving party will be blocked until the other party is ready. While with buffered channel, senders can keep sending data as long as the buffer is not full.This property can be utilized to simulate the concept of mutex to prevent critical data.Let’s take a look at an example. First we write the following program (mutex.go) using mutex:package mainimport \"fmt\"import \"sync\"func main(){ var mu sync.Mutex var wg sync.WaitGroup wg.Add(3) for i:=0; i&lt;3; i++ { go func(i int){ //Critical Section defer wg.Done() mu.Lock() for j:=0; j&lt;3; j++ { fmt.Printf(\"go routine %d , printing %d\\n\",i, j) } mu.Unlock() //End of Critical Section }(i) } wg.Wait()}Run the program we can see:go run mutex.gogo routine 2 , printing 0go routine 2 , printing 1go routine 2 , printing 2go routine 0 , printing 0go routine 0 , printing 1go routine 0 , printing 2go routine 1 , printing 0go routine 1 , printing 1go routine 1 , printing 2We can achive the same effect using a buffered channel like below:package mainimport \"fmt\"import \"sync\"func main(){ var wg sync.WaitGroup wg.Add(3) ch := make(chan int, 1) //buffer with 1 for i:=0; i&lt;3; i++ { go func(i int){ //Critical Section defer wg.Done() ch &lt;- 1 //Other routine with block for j:=0; j&lt;3; j++ { fmt.Printf(\"go routine %d , printing %d\\n\",i, j) } &lt;-ch //Other routine can now proceed //End of Critical Section }(i) } wg.Wait()}In this case, the buffered channel has a size of 1. Whenever a go routine enters the critical section, it sends a unit of data, and its job is done, the unit of data is pulled out. Thus, other go routines can proceed." }, { "title": "Go Concurrency - Write Concurrent Programs", "url": "/posts/go-concurrency-write-concurrent-programs/", "categories": "Go, Concurrency", "tags": "go, concurrency, goroutine", "date": "2022-03-17 08:55:00 -0400", "snippet": "IntroductionI have been learning Go for some time, I would like to say the concurrency feature is Go is fantastic. It makes life with concurrency program much easier. Think about most other pogramming languages, when we need concurrency, we need to create processes/threads explicitly. With Go, everything is simplied with the magic key word “go”. Another innovation in Go is the way to synchronzie go routines, channel, which sounds like an revolution from conventional methods with mutex, semaphore, etc. For every Go programmers, the proficiency of concurrency skills is an esenstial part.In this series of articles, we will explore and learn a few design paterns to design concurrent programs. Please note that this serie of articles are not for Go beginners. It is expected that readers already familar with basics of Go. OK, let’s get started.Encapsulate a concurrent taskLet’s say we want a task to run concurrently, the task will return the computation result (for simplicity, say an integer). For example, say we have a Go file test.go:package mainimport \"fmt\"import \"time\"func concurrentTask() chan int { resCh := make(chan int) go func(){ fmt.Println(\"I am doing the task, please wait\") time.Sleep(time.Second * 5) value := 99 resCh &lt;- value }() return resCh}func main() { resCh := concurrentTask() value := &lt;- resCh fmt.Printf(\"Received value from concurrent task: %d\\n\", value)}Let’s run the above Go program:$ go run test.go I am doing the task, please waitReceived value from concurrent task: 99Split a task with subtasksThe above example encapsulate a single task and created a channel to return the result. In most cases, we take advantage of concurrency to split a large computation task and use a few subtasks to accomplish the large task. In this case, we may use a single channel to aggregate results from subtasks. Let’s look another example:package mainimport \"fmt\"import \"time\"func subTask(id int, resChan chan int) { fmt.Printf(\"This is sub task %d, now doing the job..\\n\",id) time.Sleep(time.Second * 3) res := id * 10 //Just assume any computation result resChan &lt;- res}func main() { resChan := make(chan int) for i:=0; i&lt;3; i++ { //assume we have 3 subtask to do the job go subTask(i, resChan) } totalSum := 0 for i:=0; i&lt;3; i++ {//Collect result from the 3 subtask totalSum += &lt;-resChan } fmt.Printf(\"Done at main: total %d\\n\", totalSum)}$ go run test.go This is sub task 2, now doing the job..This is sub task 0, now doing the job..This is sub task 1, now doing the job..Done at main: total 30In this example, we launched three subtasks for a specific task, and the results of the subtasks are sent to a channel. Lastly, the results are agggregated (added) as the final results.SummarySee, it is very simple to write concurrent program with Go. For the first example, the main idea is to use Go routine to run the task, and use a channel to transmit the result returned from the task. For the second example, we use subtasks and their results are sent to a shared channel for final aggregation. Unlike other programming languages, basically we did not explicicty create any threads. That is the most attractive feature with Go." } ]
